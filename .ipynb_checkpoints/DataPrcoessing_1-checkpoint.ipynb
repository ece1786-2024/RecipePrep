{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uxGRAb98JNs",
    "outputId": "c71dded3-d0a0-4fcd-c4fe-2cbe2538828e"
   },
   "outputs": [],
   "source": [
    "#Uncomment below for colab\n",
    "#!pip install openai\n",
    "#!pip install pandas\n",
    "#!pip install transformers\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "# %cd  /content/drive/MyDrive/ECE1786/Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "id": "ietWo6G38nT0"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import random\n",
    "import importlib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0c4GbpPg834n"
   },
   "outputs": [],
   "source": [
    " # API+Key\n",
    "api_key_recipePrep = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Helpers.recipe_dataset_gen' from 'D:\\\\Git\\\\ECE1786\\\\RecipePrep\\\\Helpers\\\\recipe_dataset_gen.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(recipe_dataset_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Helpers.recipe_dataset_gen as recipe_dataset_helper\n",
    "#import Helpers.fussy_match_ingredient_list as fuzzy_helper\n",
    "import Helpers.food_nutrient_mapping_helpder as ingre_nut_map_helper\n",
    "import Helpers.similarity_search as sim_search_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200 #testing recipe numbers\n",
    "recipe_filename = './recipes_raw/recipes_raw_processed.json'\n",
    "recipe_dataset_small_name = f'./datasets/recipe_dataset_init_{sample_size}.json'\n",
    "long_recipe_percnt= 0.2\n",
    "processed_init_filename = f\"./datasets/processed_recipes_init_{sample_size}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the client\n",
    "\n",
    "openai.api_key=api_key_recipePrep\n",
    "temp = 0.7\n",
    "topp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_name_1 = './datasets/emb/food_descriptions_embeddings.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingre_sim_search_client = OpenAI(\n",
    "    api_key=api_key_recipePrep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_data_process_client = OpenAI(\n",
    "    api_key=api_key_recipePrep\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a smaller recipe dataset\n",
    "\n",
    "The original RecipeBox dataset contains around 100k datasets, we don't need that much for now\n",
    "\n",
    "call get_long_short_recipe_dataset() to get a smaller dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset will contains 10 long recipes and 40 short recipes.\n",
      "50 records have been saved to ./datasets/recipe_dataset_init_50.json.\n"
     ]
    }
   ],
   "source": [
    "recipe_dataset_helper.get_long_short_recipe_dataset(recipe_filename,sample_size,recipe_dataset_small_name,long_recipe_percnt=long_recipe_percnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCNG_0ekQ4Un"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "id": "l1TJiCM385KS"
   },
   "outputs": [],
   "source": [
    "def get_API_response(client,in_prompt,user_input,temp,topp):\n",
    "\n",
    "  #print(\"temperature= {0}, top_p= {1} , max_tokens={2}, input_statement={3}\".format(temp,topp,max_token,input_statement))\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": in_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    temperature = temp,\n",
    "    top_p = topp\n",
    "  )\n",
    "  response = chat_completion.choices[0].message.content\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsbO8D8aQ54N"
   },
   "source": [
    "### Recipe Data Initial Processing\n",
    "\n",
    "Parse paragraph style instructions in structured labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "id": "JEyXgTS4I9GS"
   },
   "outputs": [],
   "source": [
    "def process_API_res_get_processed_recipe(API_resonse,recipe_id,eachRecipe):\n",
    "    try:\n",
    "        processed_res= json.loads(API_resonse)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(API_resonse)\n",
    "\n",
    "    if not processed_res:  # This checks if the dictionary is empty\n",
    "        return {}  # Return an empty dictionary or handle as appropriate\n",
    "    else:\n",
    "        processed_recipe = {\n",
    "            \"recipe_id\": recipe_id,\n",
    "            \"recipe_title\": eachRecipe[\"title\"],\n",
    "            \"original_instructions\": eachRecipe[\"instructions\"],\n",
    "            \"ingredients\": eachRecipe[\"ingredients\"],\n",
    "            \"step_by_step_instructions\" : processed_res['step_by_step_instructions'],\n",
    "            \"processed_ingredients\" : processed_res['processed_ingredients'],\n",
    "            \"pure_ingredients\": processed_res[\"pure_ingredients\"],\n",
    "            \"cooking_time\": processed_res[\"cooking_time\"],\n",
    "            \"required_tools\" : processed_res[\"required_tools\"]\n",
    "        }\n",
    "    \n",
    "        return processed_recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "id": "_TkprNFRKkXE"
   },
   "outputs": [],
   "source": [
    "def get_processed_recipe_dataset(client,temp,topp,recipe_dataset,batch_size=50):\n",
    "\n",
    "    out_list = []\n",
    "    batch_counter=0\n",
    "    total_size = len(recipe_dataset)\n",
    "    for recipe_id, eachRecipe in recipe_dataset.items():\n",
    "        #pass recipe to GPT\n",
    "        ingredients_str = '. '.join(eachRecipe['ingredients'])\n",
    "        prompt_recipe_process = recipe_process_prompt.format(eachRecipe['title'],ingredients_str,eachRecipe[\"instructions\"])\n",
    "        response = get_API_response(client, in_prompt=prompt_recipe_process, user_input=\"\", temp=temp, topp=topp)\n",
    "        # print(response)\n",
    "\n",
    "        # get processed recipe & create intially processed list\n",
    "        processed_recipe =  process_API_res_get_processed_recipe(response,recipe_id,eachRecipe)\n",
    "        if not processed_recipe:\n",
    "            print(f\"Empty JSON detected. {recipe_id}: {eachRecipe['title']} is not a regular dish.\")\n",
    "        else:\n",
    "            out_list.append(processed_recipe)\n",
    "\n",
    "        #batch saving\n",
    "        if len(out_list) >= batch_size:\n",
    "            batch_counter += 1\n",
    "            processed_init_filename = f\"./datasets/processed_recipes_init_{total_size}_batch_{batch_counter}.json\"\n",
    "            with open(processed_init_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(out_list, file, indent=4)\n",
    "            \n",
    "            print(f\"Intially Processed Recipe dataset has been saved in {processed_init_filename}\")\n",
    "            out_list = [] \n",
    "\n",
    "    #remaining list\n",
    "            #batch saving\n",
    "    if len(out_list) >= 0:\n",
    "        batch_counter += 1\n",
    "        processed_init_filename = f\"./datasets/processed_recipes_init_{total_size}_batch_{batch_counter}.json\"\n",
    "        with open(processed_init_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(out_list, file, indent=4)\n",
    "        \n",
    "        print(f\"Intially Processed Recipe dataset has been saved in {processed_init_filename}\")\n",
    "            \n",
    "\n",
    "    return batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "id": "X23V0Yps-b1g"
   },
   "outputs": [],
   "source": [
    "#Get testing file\n",
    "with open(recipe_dataset_small_name, \"r\") as f:\n",
    "    recipe_dataset_TBP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "id": "4N2anRX6-ilN"
   },
   "outputs": [],
   "source": [
    "recipe_process_prompt = '''\n",
    "You are a helpful assistant that processes the following recipe:\n",
    "\n",
    "- Recipe title: {0}\n",
    "- Ingredients: {1}\n",
    "- Instructions: {2}\n",
    "\n",
    "If the recipe title indicate that this recipe is a standalone sweet dessert (e.g., ice cream), syrup, dressing, drink, or spread/topping, stop processing immediately and return an empty JSON object. Provide no additional output or explanation.\n",
    "\n",
    "**Output Requirements:**  \n",
    "The output must contain the following keys:  \n",
    "- **step_by_step_instructions:** \n",
    "    - Break the instructions into individual steps while preserving as much of the original description as possible.\n",
    "    - If the original instructions contain quantities, adjust them to reflect a single adult's portion, estimating based on ingredient totals, dish type, common serving sizes, and contextual clues (e.g., portions, instructions, or typical ingredient usage).\n",
    "- **processed_ingredients:**: \n",
    "    - Adjust quantities to a single adult's portion, estimating based on ingredient totals, dish type, common serving sizes, and contextual clues (e.g., portions, instructions, or typical ingredient usage).\n",
    "    - Convert ambiguous measurements (e.g., 'one slice') into specific measurements.\n",
    "    - Provide the converted results in a clear and consistent format while preserving the original ingredient name\n",
    "    - Remove any advertisement content from the original list. \n",
    "    - If the original recipe does not provide a measurement, include an estimation. \n",
    "    - For countable items without a unit, convert the count into an approximate weight or volume based on standard references.\n",
    "    - For ingredients given as an item number without measurement unit (e.g., 2 large potato or 1 large egg), estimate the weight/volumn based on standard average for the ingredient type. Use realistic values for common ingredient sizes and ensure the fraction is applied proportionally to the total average weight/volumn.\n",
    "    - ** It is mandatory that each processed result strictly adheres to the following format (each line must include these 3 parts): \n",
    "        - Start with a precise number, as an integer or rounded to two decimal place when reasonable. Cannot use ambious description like \"varied\" or \"enough\".\n",
    "        - Follow with a scientific measuring unit (limited to the following: tablespoon, teaspoon, ounce, cup, lb, tbsp, tsp, oz, kg, g, mg, ml, L), even for seasonings. Avoid size descriptors like 'large' or 'medium.'\n",
    "        - End with the ingredient name.**\n",
    "- **pure_ingredients:**  \n",
    "    - Extract only ingredient names from the input ingredients and instructions.\n",
    "    - For processed products (e.g., 'mashed potato'), list only the base ingredients (e.g., potato).\n",
    "    - Use generic names for ingredients with variety names (e.g., 'olives' instead of 'Kalamata olives,' 'salt' instead of 'sea salt').\n",
    "    - For ingredient mixes, use the general name directly.\n",
    "    - Each item in the list should include only one ingredient name. If a choice of ingredients is provided (e.g., 'or'), randomly select one.\n",
    "- **cooking_time:**  \n",
    "    - Total cooking time, which is the sum of all steps, including active and passive steps (e.g., preparation, cooking, waiting, or refrigeration) from the very beginning to final plating.\n",
    "    - Specify the unit of time (e.g., minutes, hours). \n",
    "- **required_tools:** List of necessary cooking tools.  \n",
    "\n",
    "The output must:\n",
    "1. Be a string in **JSON format** encoded in UTF-8.  \n",
    "2. **Exclude any code block markers** (e.g., \"```json\").  \n",
    "3. Contain only the required attributes as specified above.  \n",
    "\n",
    "Use chain of thought reasoning to process the task accurately. Validate the reasoning internally to ensure the final answer is accurate and consistent with the steps, but do not include or mention the reasoning process in the output.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty JSON detected. v1t/R3z48ITySzP2TIdxlK6PVpFyy.i: Sweet Potato Souffle is not a regular dish.\n",
      "Empty JSON detected. HNvX9BsI9Lddy/fXVMy46RUqHVQhRDG: Tex Mex Red Beans and Rice is not a regular dish.\n",
      "Empty JSON detected. 3fhwPyfHPt4EekttJZcYNx7bKlB2voa: Frozen Fruit Cups is not a regular dish.\n",
      "Empty JSON detected. 38vjeXCEVAYeptgrV8r2dbLhRxTt5Uq: Triple-Chocolate Cookie Balls is not a regular dish.\n",
      "Empty JSON detected. wu9Q0GItIfaIdpEazyHPiANI3ywiIui: California Iced Tea is not a regular dish.\n",
      "Empty JSON detected. vwGDbUIjVI1ygHvdE8PXLMEiIv4zqtS: Fried Pork T-Bone Chops with Roasted Applesauce is not a regular dish.\n",
      "Empty JSON detected. ZrF11K5//v/xcibchsJX26G.NPzzOcW: Roasted Red Bell Pepper Hummus is not a regular dish.\n",
      "Empty JSON detected. HFrmmzj1bab56NY9zjL7phWloRAeNhK: Red White Beans is not a regular dish.\n",
      "Empty JSON detected. a0FUbsNhUvdA/iIwLLhOUaqskJW.HRe: Ginger Salad is not a regular dish.\n",
      "Empty JSON detected. zC0AgiBuJ/nHIum6NfYxGpO/ISDbxBS: Black Bean Burger Panini is not a regular dish.\n",
      "Empty JSON detected. eFh9.QpEmHABlU8KLEFRdBEUdqzejbC: Lemon Lime Soda Apple Dumplings is not a regular dish.\n",
      "Empty JSON detected. 8lR8S6DpGVjhqBjLD0kcsJO961mRqfO: Polenta Pancakes with Warm Berry Sauce is not a regular dish.\n",
      "Empty JSON detected. UX3VTTxgjcO6zsjHVL.XOuSmsZUk08G: Zapperback is not a regular dish.\n",
      "Empty JSON detected. 9Pp8AZDwu.VmLseGj8M3l21VE32JXx.: Katie's Kale Smoothie is not a regular dish.\n",
      "Empty JSON detected. 6YLBCBvff.JoYh3VUwdW3FqUgHIouNC: Grilled Corn with Maple and Chipotle is not a regular dish.\n",
      "Empty JSON detected. 7o3GauBUBiOZrW0DS73i5cCuEkGDYlS: Vanilla Extract is not a regular dish.\n",
      "Empty JSON detected. znNVq1m5dWPiojg3Fnmx2NJ1MFuDnO6: Haupia (Coconut Pudding Dessert) is not a regular dish.\n",
      "Empty JSON detected. EltT1FtSwoCtAKlPkjOZgb/mhvJAjsS: Frozen Vanilla Mousse (Schiumone) is not a regular dish.\n",
      "Empty JSON detected. V6qqYx2hXyQnTSUOBcG0EPOLKMlCVKS: Italian-Style Monte Cristo Sandwiches is not a regular dish.\n",
      "Empty JSON detected. fpc5CenOsQK5bhzAbeDwHyjXjg6gbyu: Coffee Granita (with Sweetened Whipped Cream) is not a regular dish.\n",
      "Empty JSON detected. Bl9doo1kqi2RH4xVVO26ec3CeGS7kZ2: Salmon in Herb Butter is not a regular dish.\n",
      "Empty JSON detected. yyDS7w0Jd3NOPnu2slROdEWcKR3rOz6: Gina's Green Bean Salad is not a regular dish.\n",
      "Empty JSON detected. CSj/4aftxcRN9wDJdpJ.gXblzT9ibP6: Cashew Candy Crunch Sundae is not a regular dish.\n",
      "Empty JSON detected. FdEJzjkWgdKVIJzR81VzRQxjZOHjzBS: Raspberry Summer Pudding is not a regular dish.\n",
      "Empty JSON detected. aywpl88Bnq6pMAhnIDnLQdlYVs7LEfu: Sugar Snap Peas is not a regular dish.\n",
      "Empty JSON detected. apg8QJ5QTGazPYn3GvTMnI2PZM9EKza: Grilled Doughnuts with Melted Chocolate-Hazelnut Sauce is not a regular dish.\n",
      "Empty JSON detected. ZVS5.PiX3j8Qh8N7zcKlLIb3rblm/1u: Melon Balls with Honeylime Yogurt is not a regular dish.\n",
      "Empty JSON detected. EH3nb6aWqYKKvWQr0ZFAEhxU0VuQENq: Honey-Walnut Cream Cheese is not a regular dish.\n",
      "Empty JSON detected. l/CB9JIZMCr/EA7g16HS/jy.sel093S: Fudge is not a regular dish.\n",
      "Empty JSON detected. 0qLAsrXTsNiLKzbd8.LM1wcr/DPuccG: Little Radishes with Whipped Goat Cheese and Toasted Cumin is not a regular dish.\n",
      "Empty JSON detected. /rt9LTK.q4NV17gUZcpti8qeKX444MS: Mozzarella is not a regular dish.\n",
      "Empty JSON detected. /YVK5pLZ2LEZjihkGEOg3nwReUG0qSu: Hot Fudge is not a regular dish.\n",
      "Empty JSON detected. jCX1sk.rZ1NhBRtZGng6Nh3Mn4TwHS6: Herbal Iced Tea is not a regular dish.\n",
      "Empty JSON detected. abQs5Zx0tLJ8qz80JvegocHQF1WdNY6: Horchata is not a regular dish.\n",
      "Empty JSON detected. YGeaQpH8fddDP2JCAaGYzjUM3EDtsy6: Aioli is not a regular dish.\n",
      "Empty JSON detected. lM7MtUy4t/BjuI6eHp.lavLftZNNvB.: Green on Green Pasta Soup is not a regular dish.\n",
      "Intially Processed Recipe dataset has been saved in ./datasets/processed_recipes_init_200_batch_1.json\n",
      "Empty JSON detected. eCw.DId0PKt6dqZO9j.6e/SNViI4YI6: Crème Fraîche  is not a regular dish.\n",
      "Empty JSON detected. wQWbehixk4pEPO2V4qToPqYtqnG24WS: Rosemary-Sage Burgers with Apple Slaw and Chive \"Mayo\"  is not a regular dish.\n",
      "Empty JSON detected. 7GeNcasS.HPbzlEkfXJOdQD8qfcmeny: 4-Ingredient Coconut-Lime Pie is not a regular dish.\n",
      "Empty JSON detected. nnaHbkIsc8TNj7qOQPzWU0KD9PjUOOm: Saffron Mayonnaise  is not a regular dish.\n",
      "Empty JSON detected. ap58p/huBUfKVE3xwgz7OS.dr0xQ2Q.: Cran-applesauce Sundaes is not a regular dish.\n",
      "Empty JSON detected. D7v.lNSM5qzKGWNf19FRzgXWxoXvySa: Gingerbread Boys with a Bite is not a regular dish.\n",
      "Empty JSON detected. siXPqhrGta3z57a86UgTvL7IrrFjdAu: Creamy Mustard Vinaigrette for Asparagus  is not a regular dish.\n",
      "Empty JSON detected. C9wEFjJPTM6BGH4aLq0LaG8ElyP1Vhi: Buttery Apple Brown Betty is not a regular dish.\n",
      "Empty JSON detected. boZuHFZXHZV0pRJuuFXFa/wS080zLX2: Casino El Camino - Amarillo Burger is not a regular dish.\n",
      "Empty JSON detected. XBVgZT61nUJX5S9nbFvVmRsDUTFC7lm: Poached Eggs with Herb Mayonnaise is not a regular dish.\n",
      "Empty JSON detected. Y1dEgIXOGMsGxQoZ3t6Wi0LvxX/gs..: Tamarind Water is not a regular dish.\n",
      "Empty JSON detected. AWOma8VVUvyqdbMwy1zDMGm74mmnCau: Old Fashioned Pancakes is not a regular dish.\n",
      "Empty JSON detected. zPLu.pRnWeTgQ.jTmHd4QkQxWn40VTC: Lime Syrup  is not a regular dish.\n",
      "Empty JSON detected. CKPYxvrXQK/pMER91eTu3vU8J/jFgq.: Pea Champ is not a regular dish.\n",
      "Empty JSON detected. 5OB7t1RQR0EGdL/HqZ5h52vqyePTq9y: Basil Oil and Cinnamon Oil is not a regular dish.\n",
      "Empty JSON detected. 3eP9eK/uJJWhJJn.4VNpbhixi4NJgse: Cantaloupe and Cream Sherry Granita  is not a regular dish.\n",
      "Empty JSON detected. 3lQ0HKGQL8hOP1PfStlbkLa.4vnW/v2: Salt, Pepper, and Lemon Dipping Sauce  is not a regular dish.\n",
      "Empty JSON detected. RJMIM7FxXaq0.Fgq4/80RI3kSy3V6j6: Carrot Cake with Lemon Cream Cheese Frosting is not a regular dish.\n",
      "Empty JSON detected. Ac.A6wqdp5KKENN7K3wrT9YCh0YC.fa: Coconut Shrimp with Lime Dressing is not a regular dish.\n",
      "Empty JSON detected. wdgXuM1Wpwj7ABUNgKfEByIRvRJv4Py: Vanilla-Spice Butter is not a regular dish.\n",
      "Empty JSON detected. KfGhKfG37wWGxpoQb46hwGxa5Ck2eGe: Cider Gravy is not a regular dish.\n",
      "Empty JSON detected. 94SNck8q.5SYLu9QEVSbYKSHmiTZ20m: Reindeer Feed is not a regular dish.\n",
      "Empty JSON detected. DR0VGvjjhBJOIf/tev8j7ALdz8OYuoe: Nutella Panini  is not a regular dish.\n",
      "Empty JSON detected. vJphBqkbC0dBg6woudVowyUwbWIpq9S: Horseradish Mashed Potatoes is not a regular dish.\n",
      "Empty JSON detected. //KdNk5zGJdtiIxI/nzq4dNcgnJ9/YC: Cucumber and Lemon Verbena Water is not a regular dish.\n",
      "Empty JSON detected. QPpP71.vC7mMtB0OU84u5B9w4l6NGyy: Peanut Butter Macaroons is not a regular dish.\n",
      "Empty JSON detected. b1ET6RaHv5cxFDCXBYEo4viTtaYKMYm: Lemon Parsley Power Pack is not a regular dish.\n",
      "Empty JSON detected. pasAePlSDNrAz9ov9H.KzmGW6D4bwJa: Vegetable Cream Cheese is not a regular dish.\n",
      "Empty JSON detected. U8v1brpws2Tdcc.x4a17z6l.IOqV1ke: Rita's Ultra-Lite Pecan Chocolate Chip Cookies is not a regular dish.\n",
      "Empty JSON detected. hj2tiI2PdiNxP51Tx7eqrT3fhiF9AlO: Baked Apples is not a regular dish.\n",
      "Empty JSON detected. Q0IRdSvRGjiZ0ZXCnlz4.25SK/vVCje: Ginger Whipped Cream  is not a regular dish.\n",
      "Empty JSON detected. N/Gkca6YNmPI5kfUpXoUAWCVWS7PFhm: Lime Granita and Candied Lime Zest is not a regular dish.\n",
      "Empty JSON detected. cK68VDaqTEJFVgZT8dYOZMi/Yd/2mm.: Turkey Nachos is not a regular dish.\n",
      "Empty JSON detected. 81qp7NrKIRvSW9km4vUJtei5wVr8QUm: Pineapple Clafouti is not a regular dish.\n",
      "Empty JSON detected. .dUydB4Jm/PqTNefRrwWyUyHU9LufGO: Citrus Syrup  is not a regular dish.\n",
      "Empty JSON detected. lVrgdv67g1G1MPQfJ0Q8OehIhojQUAa: Truffle Pops is not a regular dish.\n",
      "Empty JSON detected. io1Hf8b3uH1ISD5Hnc05BKIUxUqsEgO: Pumpkin Ice Cream Pie with Chocolate-Almond Bark and Toffee Sauce  is not a regular dish.\n",
      "Empty JSON detected. GcqDlbl8W2pFr5YSwYt8X/KOgTJ7Hdi: Glazed Lemon Bread  is not a regular dish.\n",
      "Empty JSON detected. zgGZMZ5YyLfm5S0MT45tdZA5dYNiISm: Mussels With Spicy Tomato Oil and Grilled Bread  is not a regular dish.\n",
      "Intially Processed Recipe dataset has been saved in ./datasets/processed_recipes_init_200_batch_2.json\n",
      "Empty JSON detected. Pi4HEo3KIoofX7reV3kj/5XCEtADv9.: Spiced Layer Cake with Orange Cream Cheese Frosting  is not a regular dish.\n",
      "Empty JSON detected. 1SSZHDgmKbmwCbTO6CRkqGk6iycftiy: Grilled Knob Onions with Romesco Sauce is not a regular dish.\n",
      "Empty JSON detected. SErzqGNVBRGKsQYawdiXVIb.blppk8y: Gingerbread Cake is not a regular dish.\n",
      "Empty JSON detected. ktFIhyzLjq2HUFwEWaYgBA7YLQMfw.S: Chocolate Chip Coffee Cake  is not a regular dish.\n",
      "Empty JSON detected. fZyjHw1dge9QkFEUMMbIolgWElQmZOy: Diablo Pumpkin Seed Brittle is not a regular dish.\n",
      "Empty JSON detected. Ni1pDO2A5GN2dDVXHab5Gml99m7BwJW: Oyster-Leek Gratine with Ponzu Sabayon Kaffir Lime-Smoked Salmon Rillette is not a regular dish.\n",
      "Empty JSON detected. 798PLHo1dsSmGLAFH/p5YvziXWe9aUq: Plum Sorbet  is not a regular dish.\n",
      "Empty JSON detected. zp.SnRKMElKgjXLK8sseXSnQ1EDHS4.: Cinnamon Oatmeal Pancakes with Honey Apple Compote is not a regular dish.\n",
      "Empty JSON detected. MzgJjsApMzhI8t7c0aPu7h1gcBDbi/K: Molasses Horseradish Sweet Potato Spears  is not a regular dish.\n",
      "Intially Processed Recipe dataset has been saved in ./datasets/processed_recipes_init_200_batch_3.json\n"
     ]
    }
   ],
   "source": [
    "test_recipes = dict(recipe_dataset_TBP)\n",
    "batch_counter = get_processed_recipe_dataset(recipe_data_process_client,temp,topp,test_recipes,batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn7ndtKDQ-XG"
   },
   "source": [
    "### Ingredient-Nutrient Mapping\n",
    "\n",
    "\n",
    "Use https://produits-sante.canada.ca/api/documentation/cnf-documentation-en.html\n",
    "\n",
    "Step1: Use the food_code dataset and text-embedding-ada-002 to go similarity match, get the food_code with description that is cloest to the ingredient\n",
    "\n",
    "Step2: Use the food_code to get nutrient amount(s) \n",
    "\n",
    "Step 3: Save the mapping for the ingredient in the ingre_nutrition_map folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Get Food code description embedding\n",
    "\n",
    "This Section DON\"T NEED TO BE RUN EVERYTIME ! Otherwise the entire food code dataset will be processed -> cost \n",
    "\n",
    "Only run this section when have a new food_code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of food codes: 5690\n"
     ]
    }
   ],
   "source": [
    "food_descriptions,food_codes = sim_search_helper.get_normalized_foodCode_dataset()\n",
    "print(f'Number of food codes: {len(food_descriptions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(client,food_descriptions):        \n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=food_descriptions\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_embeddings(client, food_descriptions, batch_size=400):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(food_descriptions), batch_size):\n",
    "        batch = food_descriptions[i:i+batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1} of {len(food_descriptions) // batch_size + 1}\")\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=batch\n",
    "        )\n",
    "        embeddings.extend([item.embedding for item in response.data])\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 15\n",
      "Processing batch 2 of 15\n",
      "Processing batch 3 of 15\n",
      "Processing batch 4 of 15\n",
      "Processing batch 5 of 15\n",
      "Processing batch 6 of 15\n",
      "Processing batch 7 of 15\n",
      "Processing batch 8 of 15\n",
      "Processing batch 9 of 15\n",
      "Processing batch 10 of 15\n",
      "Processing batch 11 of 15\n",
      "Processing batch 12 of 15\n",
      "Processing batch 13 of 15\n",
      "Processing batch 14 of 15\n",
      "Processing batch 15 of 15\n",
      "Embeddings saved to ./datasets/emb/food_descriptions_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "#!!!!!\n",
    "#!!!! Only run this section when have a new food_code dataset\n",
    "#!!!!!\n",
    "food_embeddings = batch_generate_embeddings(ingre_sim_search_client, food_descriptions, batch_size=400)\n",
    "food_embeddings = np.array(food_embeddings, dtype=\"float32\")\n",
    "\n",
    "np.save(embd_name_1, food_embeddings)  # Save embeddings\n",
    "print(f\"Embeddings saved to {embd_name_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved in ./datasets/emb\\food_index.faiss\n"
     ]
    }
   ],
   "source": [
    "food_embeddings = np.load(embd_name_1)\n",
    "index = sim_search_helper.create_FAISS_Index(food_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_descriptions_ori,_ = sim_search_helper.get_normalized_foodCode_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_food_code(client, index, ingredient,top_k=30):\n",
    "\n",
    "    #Exact match\n",
    "    for idx, description in enumerate(food_descriptions):\n",
    "        if sim_search_helper.preprocess_text(ingredient) == description:\n",
    "            return food_codes[idx], description, 1.0\n",
    "\n",
    "    #Get embedding\n",
    "    contextual_input = sim_search_helper.preprocess_text(ingredient)\n",
    "    ingredient_embedding = generate_embeddings(client, [contextual_input])[0]\n",
    "\n",
    "    #Search the FAISS index for closest matches\n",
    "    distances, indices = index.search(np.array([ingredient_embedding], dtype=\"float32\"), k=top_k)\n",
    "\n",
    "    # Filter and prioritize matches\n",
    "    priority_match = None\n",
    "    best_match = None\n",
    "    best_similarity = 0\n",
    "    best_prio_similarity=0\n",
    "\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        \n",
    "        description_ori = food_descriptions_ori[idx].strip()\n",
    "        similarity = 1 - distance  # Convert distance to similarity\n",
    "\n",
    "        # Skip low-similarity matches\n",
    "        if similarity < 0.5:\n",
    "            continue\n",
    "        \n",
    "        #Case 1 example: \"Alcohol, wine, cooking\" <- where the first part is category, the second part is the actual ingredient\n",
    "        #Case 2 example: \"Butter, regular\" <- where the ingredient we are using is already the category name \n",
    "        second_part=None\n",
    "        if \",\" in description_ori:\n",
    "            parts = description_ori.lower().split(\",\")\n",
    "            first_part = parts[0].strip() if len(parts) > 0 else \"\"\n",
    "            second_part = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            second_part_words = sim_search_helper.preprocess_text(second_part).split()\n",
    "        else:\n",
    "            first_part = description_ori.lower()\n",
    "\n",
    "        ingredient_words = sim_search_helper.preprocess_text(ingredient).split()\n",
    "        first_part_words = sim_search_helper.preprocess_text(first_part).split()\n",
    "\n",
    "        #Hierachy: Check case1 first, and then case 2\n",
    "        if ingredient_words == first_part_words: \n",
    "            if similarity > best_prio_similarity:\n",
    "                priority_match = (food_codes[idx], description_ori, similarity)\n",
    "                best_prio_similarity = similarity\n",
    "    \n",
    "        elif second_part and ingredient_words == second_part_words:  # Ensure the words match exactly\n",
    "            if similarity > best_prio_similarity:\n",
    "                priority_match = (food_codes[idx], description_ori, similarity)\n",
    "                best_prio_similarity = similarity\n",
    "        \n",
    "        # Update best match\n",
    "        if similarity > best_similarity:\n",
    "            best_match = (food_codes[idx], description_ori, similarity)\n",
    "            best_similarity = similarity\n",
    "        \n",
    "    if best_similarity>0.86:\n",
    "        #special case: the non priority match result is very confident\n",
    "        return best_match\n",
    "    \n",
    "    if priority_match:\n",
    "        return priority_match\n",
    "\n",
    "    if best_match:\n",
    "        return best_match\n",
    "\n",
    "    return None, None, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./datasets\\processed_recipes_init_200_batch_1.json\n",
      "174\n",
      "Processing file: ./datasets\\processed_recipes_init_200_batch_2.json\n",
      "193\n",
      "Processing file: ./datasets\\processed_recipes_init_200_batch_3.json\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "total_item_num = len(recipe_dataset_TBP)\n",
    "\n",
    "#Get ingredients from all batch files\n",
    "folder_path = \"./datasets/\"\n",
    "batch_file_pattern = folder_path + f\"processed_recipes_init_{total_item_num}_*.json\" \n",
    "\n",
    "total_ingre_list = []\n",
    "for file_path in glob.glob(batch_file_pattern):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    batch_ingre_list = recipe_dataset_helper.get_ingre_list_from_dataset(file_path)\n",
    "    print(len(batch_ingre_list))\n",
    "    total_ingre_list.extend(batch_ingre_list)\n",
    "    \n",
    "total_ingre_list.sort()\n",
    "all_ingredients_list = set(total_ingre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food_code_for_ingredients(ingredients_list):\n",
    "    results_dict = {}\n",
    "    for ingredient in ingredients_list:\n",
    "        \n",
    "        ingredient_cleaned  = sim_search_helper.preprocess_text(ingredient)\n",
    "        #print(f\"Processing [{ingredient}] -> {ingredient_cleaned}\")\n",
    "        \n",
    "        matched_food_code, matched_food_description, similarity = find_closest_food_code(ingre_sim_search_client,index,ingredient_cleaned)\n",
    "    \n",
    "        results_dict[ingredient] = {\n",
    "            \"food_code\": matched_food_code,\n",
    "            \"description\": matched_food_description,\n",
    "            \"similarity\": similarity\n",
    "        }\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingre_food_code_result = get_food_code_for_ingredients(all_ingredients_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No Contextual Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "# Case1: with NO contectual input\n",
    "with open(\"./ingre_nutrition_map/ingre_match_.log\", \"w\") as log_file:\n",
    "    local_timestamp = datetime.now()\n",
    "    print(f\"----------------------Local Timestamp: {local_timestamp}--------------------------\", file=log_file)\n",
    "    for ingredient, details in ingre_food_code_result.items():\n",
    "\n",
    "        print(f\"Ingredient: {ingredient}\", file=log_file)\n",
    "        print(f\"Matched Description: [{details['description']}], Food Code: {details['food_code']}, Similarity: {details['similarity']:.2f}\", file=log_file)\n",
    "        print('\\n', file=log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ingredient-nutrient Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingre_food_code_result\n",
    "nut_unit_map_name = ingre_nut_map_helper.get_unitMap_name()\n",
    "ntri_unit_map = ingre_nut_map_helper.load_nut_id_map(nut_unit_map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ingredient_mapping(ingre_food_code_dict,untri_unit_map):\n",
    "    all_mapping = []\n",
    "    for eachIngre, details in ingre_food_code_dict.items():\n",
    "        match_code = details['food_code']\n",
    "   \n",
    "        map_created, untri_unit_map = ingre_nut_map_helper.get_nut_map(match_code,eachIngre,untri_unit_map)\n",
    "\n",
    "        if map_created:\n",
    "            all_mapping.append(map_created)\n",
    "        else:\n",
    "            print(f\"Ingredient: {eachIngre} - No nutrient amount found\")\n",
    "    \n",
    "    print(\"All ingredients processed!\")\n",
    "\n",
    "    return all_mapping,untri_unit_map\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ingredients processed!\n"
     ]
    }
   ],
   "source": [
    "all_mapping,untri_unit_map = get_all_ingredient_mapping(ingre_food_code_result,ntri_unit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit map ./ingre_nutrition_map/nutrient_unit_map.json updated!\n",
      "Ingredient-Nutrient mapping has been saved to ./ingre_nutrition_map\\ingredient_nutrient_map.json\n",
      "Local Timestamp: 2024-11-28 22:19:02.835369\n"
     ]
    }
   ],
   "source": [
    "#Save map\n",
    "ingre_nut_map_helper.save_nut_map(untri_unit_map,all_mapping)\n",
    "local_timestamp = datetime.now()\n",
    "print(\"Local Timestamp:\", local_timestamp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
