{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uxGRAb98JNs",
    "outputId": "c71dded3-d0a0-4fcd-c4fe-2cbe2538828e"
   },
   "outputs": [],
   "source": [
    "#Uncomment below for colab\n",
    "#!pip install openai\n",
    "#!pip install pandas\n",
    "#!pip install transformers\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "# %cd  /content/drive/MyDrive/ECE1786/Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ietWo6G38nT0"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import random\n",
    "import importlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0c4GbpPg834n"
   },
   "outputs": [],
   "source": [
    " # API+Key\n",
    "api_key_recipePrep = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Helpers.recipe_dataset_gen' from 'D:\\\\Git\\\\ECE1786\\\\RecipePrep\\\\Helpers\\\\recipe_dataset_gen.py'>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(recipe_dataset_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Helpers.recipe_dataset_gen as recipe_dataset_helper\n",
    "#import Helpers.fussy_match_ingredient_list as fuzzy_helper\n",
    "import Helpers.food_nutrient_mapping_helpder as ingre_nut_map_helper\n",
    "import Helpers.similarity_search as sim_search_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 200 #testing recipe numbers\n",
    "recipe_filename = './recipes_raw/recipes_raw_processed.json'\n",
    "recipe_dataset_small_name = f'./datasets/recipe_dataset_init_{sample_size}.json'\n",
    "long_recipe_percnt= 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the client\n",
    "\n",
    "openai.api_key=api_key_recipePrep\n",
    "temp = 1\n",
    "topp = 1\n",
    "processed_init_filename = f\"./datasets/processed_recipes_init_{sample_size}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_name_1 = './datasets/emb/food_descriptions_embeddings.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingre_sim_search_client = OpenAI(\n",
    "    api_key=api_key_recipePrep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_data_process_client = OpenAI(\n",
    "    api_key=api_key_recipePrep\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a smaller recipe dataset\n",
    "\n",
    "The original RecipeBox dataset contains around 100k datasets, we don't need that much for now\n",
    "\n",
    "call get_long_short_recipe_dataset() to get a smaller dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset will contains 40 long recipes and 160 short recipes.\n",
      "200 records have been saved to ./datasets/recipe_dataset_init_200.json.\n"
     ]
    }
   ],
   "source": [
    "recipe_dataset_helper.get_long_short_recipe_dataset(recipe_filename,sample_size,recipe_dataset_small_name,long_recipe_percnt=long_recipe_percnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCNG_0ekQ4Un"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "l1TJiCM385KS"
   },
   "outputs": [],
   "source": [
    "def get_API_response(client,in_prompt,user_input,temp,topp):\n",
    "\n",
    "  #print(\"temperature= {0}, top_p= {1} , max_tokens={2}, input_statement={3}\".format(temp,topp,max_token,input_statement))\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": in_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    temperature = temp,\n",
    "    top_p = topp\n",
    "  )\n",
    "  response = chat_completion.choices[0].message.content\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsbO8D8aQ54N"
   },
   "source": [
    "### Recipe Data Initial Processing\n",
    "\n",
    "Parse paragraph style instructions in structured labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "JEyXgTS4I9GS"
   },
   "outputs": [],
   "source": [
    "def process_API_res_get_processed_recipe(API_resonse,recipe_id,eachRecipe):\n",
    "    try:\n",
    "        processed_res= json.loads(API_resonse)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(API_resonse)\n",
    "    \n",
    "    processed_recipe = {\n",
    "        \"recipe_id\": recipe_id,\n",
    "        \"recipe_title\": eachRecipe[\"title\"],\n",
    "        \"original_instructions\": eachRecipe[\"instructions\"],\n",
    "        \"ingredients\": eachRecipe[\"ingredients\"],\n",
    "        # \"processed_output\": processed_res,\n",
    "        \"step_by_step_instructions\" : processed_res[\"step_by_step_instructions\"],\n",
    "        \"pure_ingredients\": processed_res[\"pure_ingredients\"],\n",
    "        \"cooking_time\": processed_res[\"cooking_time\"],\n",
    "        \"required_tools\" : processed_res[\"required_tools\"]\n",
    "    }\n",
    "\n",
    "    return processed_recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "_TkprNFRKkXE"
   },
   "outputs": [],
   "source": [
    "def get_processed_recipe_dataset(client,temp,topp,recipe_dataset):\n",
    "\n",
    "  out_list = []\n",
    "\n",
    "  for recipe_id, eachRecipe in recipe_dataset.items():\n",
    "    #pass recipe to GPT\n",
    "    ingredients_str = '. '.join(eachRecipe['ingredients'])\n",
    "    prompt_recipe_process = recipe_process_prompt.format(eachRecipe[\"instructions\"],ingredients_str)\n",
    "    response = get_API_response(client, in_prompt=prompt_recipe_process, user_input=\"\", temp=temp, topp=topp)\n",
    "    #print(response)\n",
    "      \n",
    "    # get processed recipe & create intially processed list\n",
    "    processed_recipe =  process_API_res_get_processed_recipe(response,recipe_id,eachRecipe)\n",
    "    out_list.append(processed_recipe)\n",
    "\n",
    "  return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "X23V0Yps-b1g"
   },
   "outputs": [],
   "source": [
    "#Get testing file\n",
    "with open(recipe_dataset_small_name, \"r\") as f:\n",
    "    recipe_dataset_TBP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "4N2anRX6-ilN"
   },
   "outputs": [],
   "source": [
    "recipe_process_prompt = '''\n",
    "You are a helpful assistant that processes the following recipe:\n",
    "\n",
    "- Instructions: {0}\n",
    "- Ingredients: {1}\n",
    "\n",
    "**Output Requirements:**  \n",
    "The output must contain the following keys:  \n",
    "- **step_by_step_instructions:** Instructions broken into individual steps.  \n",
    "- **pure_ingredients:**  \n",
    "    - Include only the ingredient names derived from the input ingredients and instructions.  \n",
    "    - For processed products (e.g., \"mashed potato\"), include only the base ingredients (e.g., potato).  \n",
    "    - For ingredients with a variety name, use only the generic name. For example, use 'olives' instead of 'Kalamata olives', use 'salt' instead of 'sea salt'  \n",
    "    - Each item in the list should contain only one ingredient name, whcih means when an instruction provides a choice of ingredients (e.g., \"or\"), randomly select one.  \n",
    "- **cooking_time:**  \n",
    "    - Include nested keys for:  \n",
    "        - **preparation:** Estimated preparation time (e.g., chopping, mixing).  \n",
    "        - **cooking:** Estimated cooking time(e.g., frying, baking).  \n",
    "        - **plating:** Estimated time for plating or decorating (e.g., garnishing).  \n",
    "    - Specify the unit of time (e.g., minutes, hours). \n",
    "- **required_tools:** List of necessary cooking tools.  \n",
    "\n",
    "The output must:\n",
    "1. Be a string in **JSON format** encoded in UTF-8.  \n",
    "2. **Exclude any code block markers** (e.g., \"```json\").  \n",
    "3. Contain only the required attributes as specified above.  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recipes = dict(recipe_dataset_TBP)\n",
    "processed_recipe_list = get_processed_recipe_dataset(recipe_data_process_client,temp,topp,test_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wNLeWg_Cfob",
    "outputId": "4ac29e8f-eebf-4ba3-fad6-c413e722cd08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intially Processed Recipe dataaset has been saved in ./datasets/processed_recipes_init_200.json\n"
     ]
    }
   ],
   "source": [
    "#save output to file\n",
    "with open(processed_init_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(processed_recipe_list, file, indent=4)\n",
    "\n",
    "print(f\"Intially Processed Recipe dataaset has been saved in {processed_init_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn7ndtKDQ-XG"
   },
   "source": [
    "### Ingredient-Nutrient Mapping\n",
    "\n",
    "\n",
    "Use https://produits-sante.canada.ca/api/documentation/cnf-documentation-en.html\n",
    "\n",
    "Step1: Use the food_code dataset and text-embedding-ada-002 to go similarity match, get the food_code with description that is cloest to the ingredient\n",
    "\n",
    "Step2: Use the food_code to get nutrient amount(s) \n",
    "\n",
    "Step 3: Save the mapping for the ingredient in the ingre_nutrition_map folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Food code description embedding\n",
    "\n",
    "This Section DON\"T NEED TO BE RUN EVERYTIME ! Otherwise the entire food code dataset will be processed -> cost \n",
    "\n",
    "Only run this section when have a new food_code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of food codes: 5690\n"
     ]
    }
   ],
   "source": [
    "food_descriptions,food_codes = sim_search_helper.get_normalized_foodCode_dataset()\n",
    "print(f'Number of food codes: {len(food_descriptions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(client,food_descriptions):        \n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=food_descriptions\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_embeddings(client, food_descriptions, batch_size=400):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(food_descriptions), batch_size):\n",
    "        batch = food_descriptions[i:i+batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1} of {len(food_descriptions) // batch_size + 1}\")\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=batch\n",
    "        )\n",
    "        embeddings.extend([item.embedding for item in response.data])\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 15\n",
      "Processing batch 2 of 15\n",
      "Processing batch 3 of 15\n",
      "Processing batch 4 of 15\n",
      "Processing batch 5 of 15\n",
      "Processing batch 6 of 15\n",
      "Processing batch 7 of 15\n",
      "Processing batch 8 of 15\n",
      "Processing batch 9 of 15\n",
      "Processing batch 10 of 15\n",
      "Processing batch 11 of 15\n",
      "Processing batch 12 of 15\n",
      "Processing batch 13 of 15\n",
      "Processing batch 14 of 15\n",
      "Processing batch 15 of 15\n",
      "Embeddings saved to ./datasets/emb/food_descriptions_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "#!!!!!\n",
    "#!!!! Only run this section when have a new food_code dataset\n",
    "#!!!!!\n",
    "food_embeddings = batch_generate_embeddings(ingre_sim_search_client, food_descriptions, batch_size=400)\n",
    "food_embeddings = np.array(food_embeddings, dtype=\"float32\")\n",
    "\n",
    "np.save(embd_name_1, food_embeddings)  # Save embeddings\n",
    "print(f\"Embeddings saved to {embd_name_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved in ./datasets/emb\\food_index.faiss\n"
     ]
    }
   ],
   "source": [
    "food_embeddings = np.load(embd_name_1)\n",
    "index = sim_search_helper.create_FAISS_Index(food_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_descriptions_ori,_ = sim_search_helper.get_normalized_foodCode_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_food_code(client, index, ingredient,top_k=30):\n",
    "\n",
    "    #Exact match\n",
    "    for idx, description in enumerate(food_descriptions):\n",
    "        if sim_search_helper.preprocess_text(ingredient) == description:\n",
    "            return food_codes[idx], description, 1.0\n",
    "\n",
    "    #Get embedding\n",
    "    contextual_input = sim_search_helper.preprocess_text(ingredient)\n",
    "    ingredient_embedding = generate_embeddings(client, [contextual_input])[0]\n",
    "\n",
    "    #Search the FAISS index for closest matches\n",
    "    distances, indices = index.search(np.array([ingredient_embedding], dtype=\"float32\"), k=top_k)\n",
    "\n",
    "    # Filter and prioritize matches\n",
    "    priority_match = None\n",
    "    best_match = None\n",
    "    best_similarity = 0\n",
    "    best_prio_similarity=0\n",
    "\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        \n",
    "        description_ori = food_descriptions_ori[idx].strip()\n",
    "        similarity = 1 - distance  # Convert distance to similarity\n",
    "\n",
    "        # Skip low-similarity matches\n",
    "        if similarity < 0.5:\n",
    "            continue\n",
    "        \n",
    "        #Case 1 example: \"Alcohol, wine, cooking\" <- where the first part is category, the second part is the actual ingredient\n",
    "        #Case 2 example: \"Butter, regular\" <- where the ingredient we are using is already the category name \n",
    "        second_part=None\n",
    "        if \",\" in description_ori:\n",
    "            parts = description_ori.lower().split(\",\")\n",
    "            first_part = parts[0].strip() if len(parts) > 0 else \"\"\n",
    "            second_part = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            second_part_words = sim_search_helper.preprocess_text(second_part).split()\n",
    "        else:\n",
    "            first_part = description_ori.lower()\n",
    "\n",
    "        ingredient_words = sim_search_helper.preprocess_text(ingredient).split()\n",
    "        first_part_words = sim_search_helper.preprocess_text(first_part).split()\n",
    "\n",
    "        #Hierachy: Check case1 first, and then case 2\n",
    "        if ingredient_words == first_part_words: \n",
    "            if similarity > best_prio_similarity:\n",
    "                priority_match = (food_codes[idx], description_ori, similarity)\n",
    "                best_prio_similarity = similarity\n",
    "    \n",
    "        elif second_part and ingredient_words == second_part_words:  # Ensure the words match exactly\n",
    "            if similarity > best_prio_similarity:\n",
    "                priority_match = (food_codes[idx], description_ori, similarity)\n",
    "                best_prio_similarity = similarity\n",
    "        \n",
    "        # Update best match\n",
    "        if similarity > best_similarity:\n",
    "            best_match = (food_codes[idx], description_ori, similarity)\n",
    "            best_similarity = similarity\n",
    "        \n",
    "    if best_similarity>0.86:\n",
    "        #special case: the non priority match result is very confident\n",
    "        return best_match\n",
    "    \n",
    "    if priority_match:\n",
    "        return priority_match\n",
    "\n",
    "    if best_match:\n",
    "        return best_match\n",
    "\n",
    "    return None, None, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ingredients_list = recipe_dataset_helper.get_ingre_list_from_dataset(processed_init_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food_code_for_ingredients(ingredients_list):\n",
    "    results_dict = {}\n",
    "    for ingredient in ingredients_list:\n",
    "        \n",
    "        ingredient_cleaned  = sim_search_helper.preprocess_text(ingredient)\n",
    "        #print(f\"Processing [{ingredient}] -> {ingredient_cleaned}\")\n",
    "        \n",
    "        matched_food_code, matched_food_description, similarity = find_closest_food_code(ingre_sim_search_client,index,ingredient_cleaned)\n",
    "    \n",
    "        results_dict[ingredient] = {\n",
    "            \"food_code\": matched_food_code,\n",
    "            \"description\": matched_food_description,\n",
    "            \"similarity\": similarity\n",
    "        }\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingre_food_code_result = get_food_code_for_ingredients(all_ingredients_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No Contextual Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "# Case1: with NO contectual input\n",
    "with open(\"./ingre_nutrition_map/ingre_match_.log\", \"w\") as log_file:\n",
    "    for ingredient, details in ingre_food_code_result.items():\n",
    "        print(f\"Ingredient: {ingredient}\", file=log_file)\n",
    "        print(f\"Matched Description: [{details['description']}], Food Code: {details['food_code']}, Similarity: {details['similarity']:.2f}\", file=log_file)\n",
    "        print('\\n', file=log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ingredient-nutrient Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingre_food_code_result\n",
    "nut_unit_map_name = ingre_nut_map_helper.get_unitMap_name()\n",
    "ntri_unit_map = ingre_nut_map_helper.load_nut_id_map(nut_unit_map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ingredient_mapping(ingre_food_code_dict,untri_unit_map):\n",
    "    all_mapping = []\n",
    "    for eachIngre, details in ingre_food_code_dict.items():\n",
    "        match_code = details['food_code']\n",
    "   \n",
    "        map_created, untri_unit_map = ingre_nut_map_helper.get_nut_map(match_code,eachIngre,untri_unit_map)\n",
    "\n",
    "        if map_created:\n",
    "            all_mapping.append(map_created)\n",
    "        else:\n",
    "            print(f\"Ingredient: {eachIngre} - No nutrient amount found\")\n",
    "    \n",
    "    print(\"All ingredients processed!\")\n",
    "\n",
    "    return all_mapping,untri_unit_map\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ingredients processed!\n"
     ]
    }
   ],
   "source": [
    "all_mapping,untri_unit_map = get_all_ingredient_mapping(ingre_food_code_result,ntri_unit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit map ./ingre_nutrition_map/nutrient_unit_map.json updated!\n",
      "Ingredient-Nutrient mapping has been saved to ./ingre_nutrition_map\\ingredient_nutrient_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save map\n",
    "ingre_nut_map_helper.save_nut_map(untri_unit_map,all_mapping)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
