{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uxGRAb98JNs",
    "outputId": "c71dded3-d0a0-4fcd-c4fe-2cbe2538828e"
   },
   "outputs": [],
   "source": [
    "#Uncomment below for colab\n",
    "#!pip install openai\n",
    "#!pip install pandas\n",
    "#!pip install transformers\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "# %cd  /content/drive/MyDrive/ECE1786/Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ietWo6G38nT0"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import random\n",
    "import importlib\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0c4GbpPg834n"
   },
   "outputs": [],
   "source": [
    " # API+Key\n",
    "api_key_recipePrep = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Helpers.recipe_dataset_gen' from 'D:\\\\Git\\\\ECE1786\\\\RecipePrep\\\\Helpers\\\\recipe_dataset_gen.py'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(recipe_dataset_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Helpers.recipe_dataset_gen as recipe_dataset_helper\n",
    "#import Helpers.fussy_match_ingredient_list as fuzzy_helper\n",
    "import Helpers.food_nutrient_mapping_helpder as ingre_nut_map_helper\n",
    "import Helpers.similarity_search as sim_search_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 250 #testing recipe numbers\n",
    "recipe_filename = './recipes_raw/recipes_raw_processed.json'\n",
    "recipe_dataset_small_name = f'./datasets/recipe_dataset_init_{sample_size}.json'\n",
    "long_recipe_percnt= 0.2\n",
    "processed_init_filename = f\"./datasets/processed_recipes_init_{sample_size}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init the client\n",
    "\n",
    "openai.api_key=api_key_recipePrep\n",
    "temp = 0.7\n",
    "topp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_name_1 = './datasets/emb/food_descriptions_embeddings.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingre_sim_search_client = OpenAI(\n",
    "    api_key=api_key_recipePrep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_data_process_client = OpenAI(\n",
    "    api_key=api_key_recipePrep\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a smaller recipe dataset\n",
    "\n",
    "The original RecipeBox dataset contains around 100k datasets, we don't need that much for now\n",
    "\n",
    "call get_long_short_recipe_dataset() to get a smaller dataset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset will contains 10 long recipes and 40 short recipes.\n",
      "50 records have been saved to ./datasets/recipe_dataset_init_50.json.\n"
     ]
    }
   ],
   "source": [
    "recipe_dataset_helper.get_long_short_recipe_dataset(recipe_filename,sample_size,recipe_dataset_small_name,long_recipe_percnt=long_recipe_percnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCNG_0ekQ4Un"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "l1TJiCM385KS"
   },
   "outputs": [],
   "source": [
    "def get_API_response(client,in_prompt,user_input,temp,topp):\n",
    "\n",
    "  #print(\"temperature= {0}, top_p= {1} , max_tokens={2}, input_statement={3}\".format(temp,topp,max_token,input_statement))\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": in_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    temperature = temp,\n",
    "    top_p = topp\n",
    "  )\n",
    "  response = chat_completion.choices[0].message.content\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsbO8D8aQ54N"
   },
   "source": [
    "### Recipe Data Initial Processing\n",
    "\n",
    "Parse paragraph style instructions in structured labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "JEyXgTS4I9GS"
   },
   "outputs": [],
   "source": [
    "def process_API_res_get_processed_recipe(API_resonse,recipe_id,eachRecipe):\n",
    "    try:\n",
    "        processed_res= json.loads(API_resonse)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(API_resonse)\n",
    "\n",
    "    if not processed_res:  # This checks if the dictionary is empty\n",
    "        return {}  # Return an empty dictionary or handle as appropriate\n",
    "    else:\n",
    "        processed_recipe = {\n",
    "            \"recipe_id\": recipe_id,\n",
    "            \"recipe_title\": eachRecipe[\"title\"],\n",
    "            \"original_instructions\": eachRecipe[\"instructions\"],\n",
    "            \"ingredients\": eachRecipe[\"ingredients\"],\n",
    "            \"step_by_step_instructions\" : processed_res['step_by_step_instructions'],\n",
    "            \"processed_ingredients\" : processed_res['processed_ingredients'],\n",
    "            \"pure_ingredients\": processed_res[\"pure_ingredients\"],\n",
    "            \"cooking_time\": processed_res[\"cooking_time\"],\n",
    "            \"required_tools\" : processed_res[\"required_tools\"]\n",
    "        }\n",
    "    \n",
    "        return processed_recipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "_TkprNFRKkXE"
   },
   "outputs": [],
   "source": [
    "def get_processed_recipe_dataset(client,temp,topp,recipe_dataset,batch_size=50):\n",
    "\n",
    "    out_list = []\n",
    "    batch_counter=0\n",
    "    total_size = len(recipe_dataset)\n",
    "    for recipe_id, eachRecipe in recipe_dataset.items():\n",
    "        #pass recipe to GPT\n",
    "        ingredients_str = '. '.join(eachRecipe['ingredients'])\n",
    "        prompt_recipe_process = recipe_process_prompt.format(eachRecipe['title'],ingredients_str,eachRecipe[\"instructions\"])\n",
    "        response = get_API_response(client, in_prompt=prompt_recipe_process, user_input=\"\", temp=temp, topp=topp)\n",
    "        # print(response)\n",
    "\n",
    "        # get processed recipe & create intially processed list\n",
    "        processed_recipe =  process_API_res_get_processed_recipe(response,recipe_id,eachRecipe)\n",
    "        if not processed_recipe:\n",
    "            print(f\"Empty JSON detected. {recipe_id}: {eachRecipe['title']} is not a regular dish.\")\n",
    "        else:\n",
    "            out_list.append(processed_recipe)\n",
    "\n",
    "        #batch saving\n",
    "        if len(out_list) >= batch_size:\n",
    "            batch_counter += 1\n",
    "            processed_init_filename = f\"./datasets/processed_recipes_init_{total_size}_batch_{batch_counter}.json\"\n",
    "            with open(processed_init_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "                json.dump(out_list, file, indent=4)\n",
    "            \n",
    "            print(f\"Intially Processed Recipe dataset has been saved in {processed_init_filename}\")\n",
    "            out_list = [] \n",
    "\n",
    "    #remaining list\n",
    "            #batch saving\n",
    "    if len(out_list) >= 0:\n",
    "        batch_counter += 1\n",
    "        processed_init_filename = f\"./datasets/processed_recipes_init_{total_size}_batch_{batch_counter}.json\"\n",
    "        with open(processed_init_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(out_list, file, indent=4)\n",
    "        \n",
    "        print(f\"Intially Processed Recipe dataset has been saved in {processed_init_filename}\")\n",
    "            \n",
    "\n",
    "    return batch_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "X23V0Yps-b1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/recipe_dataset_init_250.json\n"
     ]
    }
   ],
   "source": [
    "#Get testing file\n",
    "print(recipe_dataset_small_name)\n",
    "with open(recipe_dataset_small_name, \"r\") as f:\n",
    "    recipe_dataset_TBP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "4N2anRX6-ilN"
   },
   "outputs": [],
   "source": [
    "recipe_process_prompt = '''\n",
    "You are a helpful assistant that processes the following recipe:\n",
    "\n",
    "- Recipe title: {0}\n",
    "- Ingredients: {1}\n",
    "- Instructions: {2}\n",
    "\n",
    "If the recipe title indicate that this recipe is a standalone dessert (e.g., ice cream), syrup, dressing, drink, or spread/topping, stop processing immediately and return an empty JSON object. Provide no additional output or explanation.\n",
    "\n",
    "**Output Requirements:**  \n",
    "The output must contain the following keys:  \n",
    "- **step_by_step_instructions:** \n",
    "    - Break the instructions into individual steps while preserving as much of the original description as possible.\n",
    "    - If the original instructions contain quantities, adjust them to reflect a single adult's portion, estimating based on ingredient totals, dish type, common serving sizes, and contextual clues (e.g., portions, instructions, or typical ingredient usage).\n",
    "- **processed_ingredients:**: \n",
    "    - Adjust quantities to a single adult's portion, estimating based on ingredient totals, dish type, common serving sizes, and contextual clues (e.g., portions, instructions, or typical ingredient usage).\n",
    "    - Convert ambiguous measurements (e.g., 'one slice') into specific measurements.\n",
    "    - Provide the converted results in a clear and consistent format while preserving the original ingredient name\n",
    "    - Remove any advertisement content from the original list. \n",
    "    - If the original recipe does not provide a measurement, include an estimation. \n",
    "    - For countable items without a unit, convert the count into an approximate weight or volume based on standard references.\n",
    "    - For ingredients given as an item number without measurement unit (e.g., 2 large potato or 1 large egg), estimate the weight/volumn based on standard average for the ingredient type. Use realistic values for common ingredient sizes and ensure the fraction is applied proportionally to the total average weight/volumn.\n",
    "    - ** It is mandatory that each processed result strictly adheres to the following format (each line must include these 3 parts): \n",
    "        - Start with a precise number, as an integer or rounded to two decimal place when reasonable. Cannot use ambious description like \"varied\" or \"enough\".\n",
    "        - Follow with a scientific measuring unit (limited to the following: tablespoon, teaspoon, ounce, cup, lb, tbsp, tsp, oz, kg, g, mg, ml, L), even for seasonings. Avoid size descriptors like 'large' or 'medium.'\n",
    "        - End with the ingredient name.**\n",
    "- **pure_ingredients:**  \n",
    "    - Extract only ingredient names from the input ingredients and instructions.\n",
    "    - For processed products (e.g., 'mashed potato'), list only the base ingredients (e.g., potato).\n",
    "    - Use generic names for ingredients with variety names (e.g., 'olives' instead of 'Kalamata olives,' 'salt' instead of 'sea salt').\n",
    "    - For ingredient mixes, use the general name directly.\n",
    "    - Each item in the list should include only one ingredient name. If a choice of ingredients is provided (e.g., 'or'), randomly select one.\n",
    "- **cooking_time:**  \n",
    "    - Total cooking time, which is the sum of all steps, including active and passive steps (e.g., preparation, cooking, waiting, or refrigeration) from the very beginning to final plating.\n",
    "    - Specify the unit of time (e.g., minutes, hours). \n",
    "- **required_tools:** List of necessary cooking tools.  \n",
    "\n",
    "The output must:\n",
    "1. Be a string in **JSON format** encoded in UTF-8.  \n",
    "2. **Exclude any code block markers** (e.g., \"```json\").  \n",
    "3. Contain only the required attributes as specified above.  \n",
    "\n",
    "Use chain of thought reasoning to process the task accurately. Validate the reasoning internally to ensure the final answer is accurate and consistent with the steps, but do not include or mention the reasoning process in the output.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty JSON detected. FquvQi06MJqAd5mdH432hrWQpXaW4qi: Buttermilk Biscuits is not a regular dish.\n",
      "Empty JSON detected. Bqxr0QhFnTHuG.9Gp3HzYKJuwghE2pq: Peach Blueberry-Basil Iced Peach Tea is not a regular dish.\n",
      "Empty JSON detected. HfRXrprYiA5haGtvectCkTlwiTDnVRS: Royal Icing is not a regular dish.\n",
      "Empty JSON detected. 8/syEULCRV4HXiDY.Dpb1HyYjFlTbwe: Sesame Turkey Salad is not a regular dish.\n",
      "Empty JSON detected. 2Mwx41apwDq9VoU3crWSctK9wD/hrMW: Melon, Arugula, and Serrano Ham with Smoked Paprika Dressing is not a regular dish.\n",
      "Empty JSON detected. l0YA5DESmt.fJ2lIPiYQinh2hn1EziS: Cornbread Oyster Dressing is not a regular dish.\n",
      "Empty JSON detected. 31z1XdgqhfS9TJPY93jN0RRARWt.a52: Caramelized-Onion, Rosemary, and Pine Nut Topping  is not a regular dish.\n",
      "Empty JSON detected. BAI7BVamg0Spu0gcT.RQPeHUm8ERqJ6: Honey Mustard Slaw is not a regular dish.\n",
      "Empty JSON detected. wcOE.s.7kESzk/c3xixqOa1xYGzLOvu: Nutty Herbed Rice is not a regular dish.\n",
      "Empty JSON detected. acnwKMiiHbJNkrPZuaye2USomhI5956: Chocolate Rolls is not a regular dish.\n",
      "Empty JSON detected. Te3WJzkIz62/j8jodyV1IsP6PRUswXe: Warm Chocolate Financier with Caramelized Bananas is not a regular dish.\n",
      "Empty JSON detected. 4guevlTyQYRFcJKkxLJfiN9pHR2Zy3W: Apple Compote is not a regular dish.\n",
      "Empty JSON detected. J959DUjDT.BxCaYYn2OvF8mnRGppBb6: Finale Cheesecake is not a regular dish.\n",
      "Empty JSON detected. LvXMDkscLKztpZt6XRUloWbyu9kvutC: Horseradish Mustard is not a regular dish.\n",
      "Empty JSON detected. uktPNiAhySRkZxIQbX6JTHqDLa8HBX.: Peach-Schnapps Iced Tea is not a regular dish.\n",
      "Empty JSON detected. Oc9b8Ig2LCppxFC/ZzYv2ulwGF78aRe: Mixed Greens with Warm Pecan Dressing is not a regular dish.\n",
      "Empty JSON detected. KjKHghyJ4r0ERib41/jafI95s7Iaalq: Chocolate Chip Cheesecake Bars is not a regular dish.\n",
      "Empty JSON detected. vaZvEVuiS7r2Wdk7j4gWHa6OpTH7uJi: Jim' N Nick's Coleslaw  is not a regular dish.\n",
      "Empty JSON detected. DlVCeDfrDjhhECkmc95v4KRFYz4tPFC: Zabaglione with Mixed Fresh Berries is not a regular dish.\n",
      "Empty JSON detected. twjcNISRNXkMUq2LwGJ.KSbt01TPsMm: Easy Caesar Dressing  is not a regular dish.\n",
      "Empty JSON detected. 7.skatk3U298D.RX5my4pIge5Wdhsym: Fritter Batter is not a regular dish.\n",
      "Empty JSON detected. v0KK3.rbCEDWqMZABSc1IFwumq50awS: Almond Butter Spritz Cookies is not a regular dish.\n",
      "Empty JSON detected. Nv70nG.9moDxEPAXVVPPSfg2Ha3PicG: Fresh Tangerine Sorbet  is not a regular dish.\n",
      "Empty JSON detected. rxdAMcoUycsCd0yxg2t.jwfFwKMVtES: Asian Apple Ginger Slaw is not a regular dish.\n",
      "Empty JSON detected. wtB.mjc9BlupC3U6XGsozpzfPv7sA9q: Ceci alla Siciliana is not a regular dish.\n",
      "Intially Processed Recipe dataset has been saved in ./datasets/processed_recipes_init_250_batch_1.json\n",
      "Empty JSON detected. M8H4qizF/ufgb2uTBV7lXG3QJH535kK: Vanilla Ice Cream with Flambeed Fresh Mincemeat is not a regular dish.\n",
      "Empty JSON detected. XeH07FhX.AANXZD6IMjRTPKsk00SryO: Gochujang-Date Sauce  is not a regular dish.\n",
      "Empty JSON detected. AD9k2ijP.qPLm9vpwSpAf/La5atIlJK: Crunchy Monkey Peanut Butter-Banana Sticks is not a regular dish.\n",
      "Empty JSON detected. f5RPbmP52krWzMbfLloPQydDUIoTscK: Blueberry Balsamic Vinegar is not a regular dish.\n",
      "Empty JSON detected. YeWiJuy/ZCtPviyHS43Dy3sFD88b9CK: Complete Breakfast Smoothie is not a regular dish.\n",
      "Empty JSON detected. Xkw6c6lzdKFexayp4ffVyPkv.isCEYO: Mayonnaise is not a regular dish.\n",
      "Empty JSON detected. uKQ15KqFdwjdkQAMDNKFDF.ckSXJklG: Pecan Butter Crunch is not a regular dish.\n",
      "Empty JSON detected. ODA.L0sjilGmw8Ls4mEZx7Vbco1uFOi: Apple Soy Chai Latte is not a regular dish.\n",
      "Empty JSON detected. 4PLP1kX9P4pYkqm9P7jwjy7ltN3Ungm: Elsa's Brownies is not a regular dish.\n",
      "Empty JSON detected. 0jesOMhJtvsR2IK83WRIIUuc/BNerxm: Banana Smoothie  is not a regular dish.\n",
      "Empty JSON detected. Jdg30kdoC/TlPWFA9ulNGqK.nZN83iq: Maduros is not a regular dish.\n",
      "Empty JSON detected. tjcrRzav1UQKsfcdZmk8dmv79ZQTyJi: Basil Pecan Pesto  is not a regular dish.\n",
      "Empty JSON detected. Mqv7nUNAQyfLpd.PN5H1OH0Go/LnU.G: Yogurt-Fruit Muffins with Bran Crumb Topping is not a regular dish.\n",
      "Empty JSON detected. NKJbrUjSCFUs4eDsEUck.MMpepdRPTa: Chocolate Peanut Butter Fudge Sundae is not a regular dish.\n",
      "Empty JSON detected. 1q6P6AMKYBVK5Bqxbj.TiMDcDm6wqvW: Lighter Creamy White Bean Dip is not a regular dish.\n",
      "Empty JSON detected. 2TgyEnJFWEsZ2cl/nEZKWkAK.ZAuwyO: Paul Bunyan Cookies is not a regular dish.\n",
      "Empty JSON detected. Yc00HAQZl7yq/odBd7wiFaj24fa6P/K: Pan Seared Chilean Sea Bass, Roasted Baby Corn, and Colorful Pepper Infused Broth is not a regular dish.\n",
      "Empty JSON detected. 2fFgEGHUm32oB6aF46FVh7/A9h5bzhO: Egg Noodle is not a regular dish.\n",
      "Empty JSON detected. .pizeGSbJWJztgfFdKylqQEp5zYpOSK: Jerry's Sugared Pecans is not a regular dish.\n",
      "Empty JSON detected. qyfCtAPZ8jVaovxFAZaxpkxP.x83tx6: Blood Orange Bellini is not a regular dish.\n",
      "Empty JSON detected. 8XokbkYkYjaFFeiJRbzu1YpD/kuGFX.: Peach Crisp is not a regular dish.\n",
      "Empty JSON detected. uO56Q7MJJnb253aHSBDo80.DYT2u1Pa: Vanilla American Buttercream is not a regular dish.\n",
      "Empty JSON detected. H2hGb1xkU7LkVNxPS1DF0399sDPS36u: Chocolate Chip Peanut Butter Cookies is not a regular dish.\n",
      "Empty JSON detected. 4Cv4hEXEIubKk6idICD.76UO4MKtvL.: Almond-Coconut Granola is not a regular dish.\n",
      "Empty JSON detected. fnVZftajtpMjZBN1WXxeZFx19BwAlYK: Come and Break It More is not a regular dish.\n",
      "Empty JSON detected. s1Oy7uT8ucYH9RS7kpzE92p9hE.7SM6: Sauteed Chicken Breast with Clover Honey and Chili is not a regular dish.\n",
      "Empty JSON detected. NXWywq.anoKnSIJfAXjzEtYnaFULj46: Make-Me-Crazy Grill Marinade  is not a regular dish.\n",
      "Empty JSON detected. W7Qz/drHF8y.BBqilJYAlbFVKMOxPVW: Nutcracker Cones is not a regular dish.\n",
      "Empty JSON detected. 9L1CzSYkr1DIx6S4CwEddtQnfDubt5q: Hot Whiskey Toddy  is not a regular dish.\n",
      "Empty JSON detected. t7Kbc9C/YuflyR0OKm4xF7ERVwhCAPu: VELVEETA Easy Cheesy Fajita Dip is not a regular dish.\n",
      "Empty JSON detected. 7nxRmuiu5djjrkP575rG7SyxjPvHVUG: Lemon Confit is not a regular dish.\n",
      "Empty JSON detected. gVZ5pFcngTvlSO9qiJVKoIiQJ1UYUAK: Creamy Basil Dressing  is not a regular dish.\n",
      "Empty JSON detected. uT6McC3NyXfs6C7uHkHAGZ0MUhJjboe: Peppers and Parmesan Cheese is not a regular dish.\n",
      "Empty JSON detected. 4csrPcvG5uOdT/95bRlFDO6Ul175PrS: Hazelnut Crusted Duck Breast is not a regular dish.\n",
      "Empty JSON detected. n7wMrgFtB/fYekQJbDEvPrWFnq.aFNi: Zucchini Cakes with Herb Sour Cream is not a regular dish.\n",
      "Empty JSON detected. aG3GSCIr18V/SoMG4gwxh8pUwv/jLam: Simple Lemon Dressing  is not a regular dish.\n",
      "Empty JSON detected. Sbp3hcC1qMsrntbyBA8ssoIt2xP608S: Cinnamon Rolls is not a regular dish.\n",
      "Intially Processed Recipe dataset has been saved in ./datasets/processed_recipes_init_250_batch_2.json\n",
      "Empty JSON detected. ahU4nqKzJ8J.de7nAsQjztjTELlqb3y: Scallion and Ginger Sauce is not a regular dish.\n",
      "Empty JSON detected. qZvRyYDHtKnhfesgwGOXRBpMwjAVgUW: Easy Cheesy Spinach is not a regular dish.\n",
      "Empty JSON detected. 867eBCr/ZSgePwQC15bmVT8zg9CnDvy: Sumac-Garlic Mayonnaise is not a regular dish.\n",
      "Empty JSON detected. 5QXSKrWklV25STFetbMRmhGCD7UdIYi: Deviled Eggs is not a regular dish.\n",
      "Empty JSON detected. argiSHD0Ntr3yWr4SP4QVbH7akgl3Om: Mexican Coffee Milkshake is not a regular dish.\n",
      "Empty JSON detected. lfE4UzsKeIA35hAliGaKGzb61jGmnGy: Vanilla Brown Sugar Syrup  is not a regular dish.\n",
      "Empty JSON detected. KuDlD5oYCqIznu2zR4hq/tZYiX03dhS: Peach Fritters with Orange Glaze is not a regular dish.\n",
      "Empty JSON detected. MZ97z5BMF/VdNDp8d1LGRsqMNToWqtO: Easiest Ever French Toast is not a regular dish.\n",
      "Empty JSON detected. Caz0YAyMIzC8GYHO6AXuiHu5Ev0ISCS: Arugula and Pear Salad is not a regular dish.\n",
      "Empty JSON detected. JkWKX51BX.LC6Lg4gbVfiKC9QPB269q: Chili Mayonnaise Shrimp Cocktail Sauce is not a regular dish.\n",
      "Empty JSON detected. bVfqBgHz1.ByuvyqtC6Zy1LzBgAc35C: Lemon Cookies  is not a regular dish.\n",
      "Empty JSON detected. Hm5XnJI7PAQnAAdTQhqhd.viqM8qgBK: Shortcake Club is not a regular dish.\n",
      "Empty JSON detected. 7xvwhbJmTOV6V8UdkBkygfR7bSifVI2: Pesto-Tomato Clams  is not a regular dish.\n",
      "Empty JSON detected. 7u0V8dxl5r6Gdgc.64C51Ts4111PAdy: Red Caviar Dip  is not a regular dish.\n",
      "Empty JSON detected. 1wIWODuhQjgD2PYHkp3EH9YfQMhDs0C: Tarragon Pan Sauce is not a regular dish.\n",
      "Empty JSON detected. IAht2X0S7bxmCGAi5ZAnzqb/tZ00U/G: Garlic Mayonnaise  is not a regular dish.\n",
      "Empty JSON detected. CGdBAYpTd6OjsWSUqmDSONowjedNIc6: Blueberry Coffeecake French Toast with Fresh Blueberries is not a regular dish.\n",
      "Empty JSON detected. 1YIJgQQ662OU0RdFV352KUUgVyNK7p6: Poached Bamboo Shrimp is not a regular dish.\n",
      "Empty JSON detected. 8R3J3c3mtKxDz./K7U/mpxg08te4Hyq: Crepes Suzette with Vanilla Ice Cream and Orange Butter Sauce is not a regular dish.\n",
      "Empty JSON detected. 7j6eEhAsTM/rE4rAxKlGxEOyOH8II2G: Fregolata: Crumb Cookie (Italy) is not a regular dish.\n",
      "Empty JSON detected. f.rswSblRNzid6WSjVJPjySBJrCSjbu: Salted Caramel Chocolate Chip Cookies is not a regular dish.\n",
      "Empty JSON detected. nSSNhs.taPlG.Of0q2oyzvRc95lTbjy: Cracked Pepper Potato Chips with Onion Dip is not a regular dish.\n",
      "Empty JSON detected. hvhQI1tX4G.lf7Z9C7damsJgLDEAEaC: Pistachio Melbas (Australia) is not a regular dish.\n",
      "Empty JSON detected. JEQpcWqsRSAuEXh6ZNxUitAM7uXmvS6: Shoofly Pie is not a regular dish.\n",
      "Empty JSON detected. 3lkVagw1BqfoUmix1pzMn73fxSMUxb.: Chile and Cheese Rice is not a regular dish.\n",
      "Empty JSON detected. 3NQWV/T.yCkCOiuSf4.0c2IBYA/9Opu: Fresh-Herb Dumplings  is not a regular dish.\n",
      "Empty JSON detected. pbFgZ7zNYq4Bh5ZEulBLAd3pb1exPv.: Chicken Breasts with Sun-Dried Tomato and Garlic Crust  is not a regular dish.\n",
      "Empty JSON detected. QIc0hygVhvhL9rv4z/deMfitDQf0C8G: Pumpkin Flan is not a regular dish.\n",
      "Empty JSON detected. IgfxfdI6.388eetLcdOqIENkrQU4cSG: Sugar Cookie with Orange-Cinnamon Icing is not a regular dish.\n",
      "Empty JSON detected. Zss0t.mZEEXhYbTBVIFivM/F0WtJ732: Yogurt Cake with Marmalade Glaze  is not a regular dish.\n",
      "Empty JSON detected. yARR2Kl95UJd9JrxKufo7M20itRCHT2: Rosemary Grilled Tuna Steaks with Eggplant and Zucchini is not a regular dish.\n",
      "Empty JSON detected. 1LHHjlfy86y.XP8RFF6lckN8Cbcpyb2: Grilled Sausage Patties with Peppers and Onions is not a regular dish.\n",
      "Empty JSON detected. PtRrRmxrF/drDk6Oy1qE.zgXjzYxVRO: Grilled Corn with Hoisin-Orange Butter  is not a regular dish.\n",
      "Empty JSON detected. QOEsrr7e4xyenlu644PJYoN6D74G0MK: Pumpkin Cheesecake is not a regular dish.\n",
      "Empty JSON detected. r5GoTkRa/jElWHoltiKYIuWpXBzqgHu: Pesche alla Nonna is not a regular dish.\n",
      "Empty JSON detected. DIoCn1h5LMELu10aLkCwSm3iYv24AJW: Chocolate Souffle is not a regular dish.\n",
      "Empty JSON detected. UZdfI3SPcHTTSh/gsvJP5KI/aK2/V3K: Pesto is not a regular dish.\n",
      "Empty JSON detected. Qni2Ipv6h5T07/82XMgsKpQaG8Nz/iq: Shrimp Risotto with Fresh Herb Butter  is not a regular dish.\n",
      "Empty JSON detected. JMMIHqCW7ymNckuDSbTCDIGNAO62cpa: Dried Fig-Smoked Almond Bark is not a regular dish.\n",
      "Empty JSON detected. s0aFKYlW0sjBFj9/kPfmgtl9mNNRxyy: Beets with Goat Cheese, Nigella Seeds, and Pistachios  is not a regular dish.\n",
      "Empty JSON detected. qA2pRhpFF7V.qf/IQVHEmw.obqbSOhS: Carrot Cake Cupcakes is not a regular dish.\n",
      "Empty JSON detected. gC7xKMuRf1oE3ZfOwQUoLSgV0leZHHe: Louisiana Shrimp Rice Dressing  is not a regular dish.\n",
      "Empty JSON detected. DN1W8mrrl.FbhagwV8409D86S3x1bgC: Potatoes and Leeks is not a regular dish.\n",
      "Intially Processed Recipe dataset has been saved in ./datasets/processed_recipes_init_250_batch_3.json\n"
     ]
    }
   ],
   "source": [
    "test_recipes = dict(recipe_dataset_TBP)\n",
    "batch_counter = get_processed_recipe_dataset(recipe_data_process_client,temp,topp,test_recipes,batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zn7ndtKDQ-XG"
   },
   "source": [
    "### Ingredient-Nutrient Mapping\n",
    "\n",
    "\n",
    "Use https://produits-sante.canada.ca/api/documentation/cnf-documentation-en.html\n",
    "\n",
    "Step1: Use the food_code dataset and text-embedding-ada-002 to go similarity match, get the food_code with description that is cloest to the ingredient\n",
    "\n",
    "Step2: Use the food_code to get nutrient amount(s) \n",
    "\n",
    "Step 3: Save the mapping for the ingredient in the ingre_nutrition_map folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Food code description embedding\n",
    "\n",
    "This Section DON\"T NEED TO BE RUN EVERYTIME ! Otherwise the entire food code dataset will be processed -> cost \n",
    "\n",
    "Only run this section when have a new food_code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of food codes: 5690\n"
     ]
    }
   ],
   "source": [
    "food_descriptions,food_codes = sim_search_helper.get_normalized_foodCode_dataset()\n",
    "print(f'Number of food codes: {len(food_descriptions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(client,food_descriptions):        \n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=food_descriptions\n",
    "    )\n",
    "    return [item.embedding for item in response.data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generate_embeddings(client, food_descriptions, batch_size=400):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(food_descriptions), batch_size):\n",
    "        batch = food_descriptions[i:i+batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1} of {len(food_descriptions) // batch_size + 1}\")\n",
    "        response = client.embeddings.create(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            input=batch\n",
    "        )\n",
    "        embeddings.extend([item.embedding for item in response.data])\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 of 15\n",
      "Processing batch 2 of 15\n",
      "Processing batch 3 of 15\n",
      "Processing batch 4 of 15\n",
      "Processing batch 5 of 15\n",
      "Processing batch 6 of 15\n",
      "Processing batch 7 of 15\n",
      "Processing batch 8 of 15\n",
      "Processing batch 9 of 15\n",
      "Processing batch 10 of 15\n",
      "Processing batch 11 of 15\n",
      "Processing batch 12 of 15\n",
      "Processing batch 13 of 15\n",
      "Processing batch 14 of 15\n",
      "Processing batch 15 of 15\n",
      "Embeddings saved to ./datasets/emb/food_descriptions_embeddings.npy\n"
     ]
    }
   ],
   "source": [
    "#!!!!!\n",
    "#!!!! Only run this section when have a new food_code dataset\n",
    "#!!!!!\n",
    "food_embeddings = batch_generate_embeddings(ingre_sim_search_client, food_descriptions, batch_size=400)\n",
    "food_embeddings = np.array(food_embeddings, dtype=\"float32\")\n",
    "\n",
    "np.save(embd_name_1, food_embeddings)  # Save embeddings\n",
    "print(f\"Embeddings saved to {embd_name_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved in ./datasets/emb\\food_index.faiss\n"
     ]
    }
   ],
   "source": [
    "food_embeddings = np.load(embd_name_1)\n",
    "index = sim_search_helper.create_FAISS_Index(food_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_descriptions_ori,_ = sim_search_helper.get_regular_foodCode_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_food_code(client, index, ingredient,top_k=30):\n",
    "\n",
    "    #Exact match\n",
    "    for idx, description in enumerate(food_descriptions):\n",
    "        if sim_search_helper.preprocess_text(ingredient) == description:\n",
    "            return food_codes[idx], description, 1.0\n",
    "\n",
    "    #Get embedding\n",
    "    contextual_input = sim_search_helper.preprocess_text(ingredient)\n",
    "    ingredient_embedding = generate_embeddings(client, [contextual_input])[0]\n",
    "\n",
    "    #Search the FAISS index for closest matches\n",
    "    distances, indices = index.search(np.array([ingredient_embedding], dtype=\"float32\"), k=top_k)\n",
    "\n",
    "    # Filter and prioritize matches\n",
    "    priority_match = None\n",
    "    best_match = None\n",
    "    best_similarity = 0\n",
    "    best_prio_similarity=0\n",
    "    \n",
    "    first_part_empty = True\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        \n",
    "        description_ori = food_descriptions_ori[idx].strip()\n",
    "        similarity = 1 - distance  # Convert distance to similarity\n",
    "        #print(f'-------{description_ori},sim={similarity}')\n",
    "        # Skip low-similarity matches\n",
    "        if similarity < 0.5:\n",
    "            continue\n",
    "        \n",
    "        #Case 1 example: \"Alcohol, wine, cooking\" <- where the first part is category, the second part is the actual ingredient\n",
    "        #Case 2 example: \"Butter, regular\" <- where the ingredient we are using is already the category name \n",
    "        second_part=None\n",
    "        if \",\" in description_ori:\n",
    "            parts = description_ori.lower().split(\",\")\n",
    "            first_part = parts[0].strip() if len(parts) > 0 else \"\"\n",
    "            second_part = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            second_part_words = sim_search_helper.preprocess_text(second_part).split()\n",
    "        else:\n",
    "            first_part = description_ori.lower()\n",
    "\n",
    "        ingredient_words = sim_search_helper.preprocess_text(ingredient).split()\n",
    "        first_part_words = sim_search_helper.preprocess_text(first_part).split()\n",
    "\n",
    "\n",
    "        #Hierachy: Check case1 first, and then case 2\n",
    "        if ingredient_words == first_part_words: \n",
    "            #print(f'evaluataing:{first_part_empty}, {best_prio_similarity-similarity}')\n",
    "            if similarity > best_prio_similarity or (first_part_empty and best_prio_similarity-similarity<0.1):\n",
    "                priority_match = (food_codes[idx], description_ori, similarity)\n",
    "                best_prio_similarity = similarity\n",
    "                #print('First part chosen')\n",
    "                first_part_empty=False\n",
    "    \n",
    "        elif second_part and ingredient_words == second_part_words:  # Ensure the words match exactly\n",
    "            if similarity > best_prio_similarity:\n",
    "                priority_match = (food_codes[idx], description_ori, similarity)\n",
    "                best_prio_similarity = similarity\n",
    "                #print('Second part chosen')\n",
    "        \n",
    "        # Update best match\n",
    "        if similarity > best_similarity:\n",
    "            best_match = (food_codes[idx], description_ori, similarity)\n",
    "            best_similarity = similarity\n",
    "            #print('Best Sim chosen')\n",
    "        \n",
    "    if priority_match:\n",
    "       # print('Prio Match')\n",
    "        return priority_match\n",
    "\n",
    "    if best_match:\n",
    "        #print('Best Match')\n",
    "        return best_match\n",
    "\n",
    "    return None, None, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Full list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_200_batch_1.json\n",
      "174\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_200_batch_2.json\n",
      "193\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_200_batch_3.json\n",
      "100\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_250_batch_1.json\n",
      "183\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_250_batch_2.json\n",
      "197\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_250_batch_3.json\n",
      "195\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_400_batch_1.json\n",
      "192\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_400_batch_2.json\n",
      "171\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_400_batch_3.json\n",
      "183\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_400_batch_4.json\n",
      "193\n",
      "Processing file: ./datasets/Processed_Recipes\\processed_recipes_init_400_batch_5.json\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "total_item_num = len(recipe_dataset_TBP)\n",
    "\n",
    "#Get ingredients from all batch files\n",
    "folder_path = \"./datasets/Processed_Recipes/\"\n",
    "batch_file_pattern = folder_path + f\"processed_recipes_init*.json\" \n",
    "\n",
    "total_ingre_list = []\n",
    "for file_path in glob.glob(batch_file_pattern):\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    batch_ingre_list = recipe_dataset_helper.get_ingre_list_from_dataset(file_path)\n",
    "    print(len(batch_ingre_list))\n",
    "    total_ingre_list.extend(batch_ingre_list)\n",
    "    \n",
    "total_ingre_list.sort()\n",
    "all_ingredients_list = set(total_ingre_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n"
     ]
    }
   ],
   "source": [
    "all_ingredients_list = list(set(all_ingredients_list))\n",
    "all_ingredients_list.sort()\n",
    "print(len(all_ingredients_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food_code_for_ingredients(ingredients_list):\n",
    "    results_dict = {}\n",
    "    for ingredient in ingredients_list:\n",
    "        \n",
    "        ingredient_cleaned  = sim_search_helper.preprocess_text(ingredient)\n",
    "        #print(f\"Processing [{ingredient}] -> {ingredient_cleaned}\")\n",
    "        \n",
    "        matched_food_code, matched_food_description, similarity = find_closest_food_code(ingre_sim_search_client,index,ingredient_cleaned)\n",
    "    \n",
    "        results_dict[ingredient] = {\n",
    "            \"food_code\": matched_food_code,\n",
    "            \"description\": matched_food_description,\n",
    "            \"similarity\": similarity\n",
    "        }\n",
    "    return results_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingre_food_code_result = get_food_code_for_ingredients(all_ingredients_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Tomato powder,sim=0.7841540575027466\n",
      "-------Tomato, sun-dried,sim=0.7747030705213547\n",
      "evaluataing:True, -0.7747030705213547\n",
      "First part chosen\n",
      "-------Tomato, yellow, raw,sim=0.7696787416934967\n",
      "evaluataing:False, 0.005024328827857971\n",
      "-------Tomato ketchup (catsup),sim=0.764974445104599\n",
      "-------Tomato, green, raw,sim=0.7642398029565811\n",
      "evaluataing:False, 0.01046326756477356\n",
      "-------Tomato, crushed, canned,sim=0.7627696245908737\n",
      "evaluataing:False, 0.011933445930480957\n",
      "-------Tomatillo, raw,sim=0.7599292099475861\n",
      "-------Tomato, orange, raw,sim=0.75562684237957\n",
      "evaluataing:False, 0.019076228141784668\n",
      "-------Tomato, red, ripe, boiled,sim=0.7370864748954773\n",
      "evaluataing:False, 0.03761659562587738\n",
      "-------Tomato, red, ripe, stewed (scalloped with breadcrumbs),sim=0.7362608909606934\n",
      "evaluataing:False, 0.038442179560661316\n",
      "-------Soup, tomato, ready-to-serve,sim=0.7338624000549316\n",
      "-------Tomato, red, ripe, canned, stewed,sim=0.7331613004207611\n",
      "evaluataing:False, 0.04154177010059357\n",
      "-------Juice, tomato, canned,sim=0.7325571179389954\n",
      "-------Tomato, red, ripe, boiled, with salt,sim=0.7297155261039734\n",
      "evaluataing:False, 0.04498754441738129\n",
      "-------Tomato products, canned, sauce,sim=0.7265282273292542\n",
      "-------Tomato, red, ripe, canned with green chilies,sim=0.7214470207691193\n",
      "evaluataing:False, 0.05325604975223541\n",
      "-------Tomato products, canned, sauce with onions, green peppers and celery,sim=0.7197542190551758\n",
      "-------Tomato products, canned, sauce with herbs and cheese,sim=0.717875063419342\n",
      "-------Salad dressing, bacon and tomato,sim=0.7160155177116394\n",
      "-------Tomato products, canned, sauce, with onions,sim=0.7153987288475037\n",
      "-------Tomato products, canned, sauce with mushrooms,sim=0.7145915329456329\n",
      "-------Spaghetti with pomodoro sauce (tomato sauce),sim=0.712753027677536\n",
      "-------Tomato products, canned, sauce with tomato tidbits,sim=0.7055945992469788\n",
      "-------Tomato, red, ripe, canned, whole,sim=0.7041419744491577\n",
      "evaluataing:False, 0.07056109607219696\n",
      "-------Soup, tomato and red pepper, ready-to-serve, low sodium,sim=0.7040455937385559\n",
      "-------Soup, tomato, dehydrated,sim=0.7024749517440796\n",
      "-------Soup, tomato, canned, condensed,sim=0.7016250193119049\n",
      "-------Soup, tomato vegetable, dehydrated,sim=0.6992597579956055\n",
      "-------Juice, tomato and vegetable, low sodium,sim=0.6940847635269165\n",
      "-------Squash, summer, zucchini in tomato juice, canned,sim=0.6897971630096436\n",
      "====Tomato, sun-dried\n"
     ]
    }
   ],
   "source": [
    "test_food = 'tomato'\n",
    "matched_food_code, matched_food_description, similarity = find_closest_food_code(ingre_sim_search_client,index,test_food)\n",
    "print(f'===={matched_food_description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No Contextual Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "# Case1: with NO contectual input\n",
    "with open(\"./ingre_nutrition_map/ingre_match_.log\", \"w\") as log_file:\n",
    "    local_timestamp = datetime.now()\n",
    "    print(f\"----------------------Local Timestamp: {local_timestamp}--------------------------\", file=log_file)\n",
    "    for ingredient, details in ingre_food_code_result.items():\n",
    "\n",
    "        print(f\"Ingredient: {ingredient}\", file=log_file)\n",
    "        print(f\"Matched Description: [{details['description']}], Food Code: {details['food_code']}, Similarity: {details['similarity']:.2f}\", file=log_file)\n",
    "        print('\\n', file=log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ingredient-nutrient Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ingre_food_code_result\n",
    "nut_unit_map_name = ingre_nut_map_helper.get_unitMap_name()\n",
    "ntri_unit_map = ingre_nut_map_helper.load_nut_id_map(nut_unit_map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_ingredient_mapping(ingre_food_code_dict,untri_unit_map):\n",
    "    all_mapping = []\n",
    "    for eachIngre, details in ingre_food_code_dict.items():\n",
    "        match_code = details['food_code']\n",
    "   \n",
    "        map_created, untri_unit_map = ingre_nut_map_helper.get_nut_map(match_code,eachIngre,untri_unit_map)\n",
    "\n",
    "        if map_created:\n",
    "            all_mapping.append(map_created)\n",
    "        else:\n",
    "            print(f\"Ingredient: {eachIngre} - No nutrient amount found\")\n",
    "    \n",
    "    print(\"All ingredients processed!\")\n",
    "\n",
    "    return all_mapping,untri_unit_map\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ingredients processed!\n"
     ]
    }
   ],
   "source": [
    "all_mapping,untri_unit_map = get_all_ingredient_mapping(ingre_food_code_result,ntri_unit_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit map ./ingre_nutrition_map/nutrient_unit_map.json updated!\n",
      "Ingredient-Nutrient mapping has been saved to ./ingre_nutrition_map\\ingredient_nutrient_map.json\n",
      "Local Timestamp: 2024-12-07 21:30:34.209851\n"
     ]
    }
   ],
   "source": [
    "#Save map\n",
    "ingre_nut_map_helper.save_nut_map(untri_unit_map,all_mapping)\n",
    "local_timestamp = datetime.now()\n",
    "print(\"Local Timestamp:\", local_timestamp)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
